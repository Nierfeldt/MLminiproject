{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "424a8742",
   "metadata": {},
   "source": [
    "\n",
    "# Q-Learning on a Custom FrozenLake Environment\n",
    "In this notebook, we explore the application of Q-learning to train an agent that navigates a custom 11x11 FrozenLake environment.\n",
    "We aim to help the agent learn to avoid holes and reach the goal with an optimal policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea904ad",
   "metadata": {},
   "source": [
    "\n",
    "## Environment Setup\n",
    "- **Custom Map**: A customized 11x11 Frozen Lake map with start (S), holes (H), frozen tiles (F), and goal (G).\n",
    "- **Environment Initialization**: We set `is_slippery=False` to remove randomness in movements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337da7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import venv\n",
    "# Create a virtual environment\n",
    "#venv.create('env', with_pip=True)\n",
    "#!env\\Scripts\\activate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea17d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate the virtual environment and install dependencies\n",
    "!pip install gymnasium gymnasium[toy_text] matplotlib \"numpy<2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c45a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Fixed 11x11 custom map\n",
    "custom_map = [\n",
    "    \"SFHFFFHFFFF\",\n",
    "    \"HFFFFFFFFFF\",\n",
    "    \"FFFHFFHFFFF\",\n",
    "    \"FFFFFFFFFFF\",\n",
    "    \"FFFFHFFFFFH\",\n",
    "    \"FHFFFFFFHFF\",\n",
    "    \"FFFHFFFFFFF\",\n",
    "    \"FFFFFHFFFFF\",\n",
    "    \"FFHFFFFFFFF\",\n",
    "    \"HFFFFFFFFFF\",\n",
    "    \"FFFFFFFFFFG\"\n",
    "]\n",
    "\n",
    "# Create Frozen Lake environment\n",
    "env = gym.make(\"FrozenLake-v1\", desc=custom_map, is_slippery=False, render_mode=\"rgb_array\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1683c7b4",
   "metadata": {},
   "source": [
    "\n",
    "## Q-Table Initialization\n",
    "- The Q-table stores learned values for each (state, action) pair.\n",
    "- Initialized as zeros, representing equal preference for all actions initially.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f89be7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the Q-table\n",
    "state_space = env.observation_space.n\n",
    "action_space = env.action_space.n\n",
    "q_table = np.zeros((state_space, action_space))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c2e3b9",
   "metadata": {},
   "source": [
    "\n",
    "## Hyperparameters Setup\n",
    "- **Alpha**: Learning rate controls how much new knowledge overrides the old.\n",
    "- **Gamma**: Discount factor for future rewards.\n",
    "- **Epsilon**: Initial exploration rate for epsilon-greedy strategy.\n",
    "- **Epsilon Decay**: Reduces exploration over time.\n",
    "- **Episodes & Steps**: Control the duration of training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cd3f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.8  # Learning rate\n",
    "gamma = 0.95  # Discount factor\n",
    "epsilon = 0.1  # Initial exploration rate\n",
    "epsilon_decay = 0.01  # Epsilon decay rate\n",
    "min_epsilon = 0.1  # Minimum epsilon\n",
    "episodes = 30000  # Number of episodes for training\n",
    "max_steps = 300  # Max steps per episode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73846426",
   "metadata": {},
   "source": [
    "\n",
    "## Loading or Initializing Q-Table\n",
    "Load an existing Q-table if available to continue learning, otherwise start fresh.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4945b15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load Q-table if it exists\n",
    "try:\n",
    "    q_table = np.load(\"frozenlake_qtable.npy\")\n",
    "    print(\"Loaded saved Q-table.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No saved Q-table found. Starting fresh.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f87c9a6",
   "metadata": {},
   "source": [
    "\n",
    "## Training Loop\n",
    "The training loop where the agent learns by interacting with the environment:\n",
    "- **Epsilon-Greedy**: Chooses between exploration and exploitation.\n",
    "- **Rewards Structure**: Rewards based on reaching the goal, falling in a hole, or taking steps.\n",
    "- **Q-Update**: Updates the Q-values based on the observed rewards and future potential rewards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f00a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "episode_rewards = []\n",
    "states_visited = []\n",
    "actions_taken = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    total_rewards = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state, :])\n",
    "        \n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        goal_state = state_space - 1\n",
    "        distance_to_goal = abs(state - goal_state)\n",
    "        next_distance_to_goal = abs(next_state - goal_state)\n",
    "\n",
    "        if done and reward == 0:\n",
    "            reward = -100\n",
    "        elif done and reward == 1:\n",
    "            reward = 100\n",
    "        else:\n",
    "            step_penalty = -10\n",
    "            reward = step_penalty + 10 * (distance_to_goal - next_distance_to_goal)\n",
    "\n",
    "        q_table[state, action] = q_table[state, action] + alpha * (\n",
    "            reward + gamma * np.max(q_table[next_state, :]) - q_table[state, action]\n",
    "        )\n",
    "\n",
    "        states_visited.append(state)\n",
    "        actions_taken.append(action)\n",
    "        state = next_state\n",
    "        total_rewards += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "    episode_rewards.append(total_rewards)\n",
    "\n",
    "    if (episode + 1) % 500 == 0:\n",
    "        print(f\"Episode {episode + 1} | Epsilon: {epsilon:.4f} | Last Reward: {total_rewards}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aff8b0",
   "metadata": {},
   "source": [
    "\n",
    "## Saving Q-Table\n",
    "Saves the trained Q-table to continue learning or for future usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97486c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the trained Q-table\n",
    "np.save(\"frozenlake_qtable.npy\", q_table)\n",
    "print(\"Trained Q-table saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6ee16c",
   "metadata": {},
   "source": [
    "\n",
    "## Testing the Agent\n",
    "After training, we test the agent to evaluate its performance based on the learned policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59409967",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Testing the agent\n",
    "test_episodes = 100\n",
    "total_rewards = 0\n",
    "\n",
    "for episode in range(test_episodes):\n",
    "    state, _ = env.reset()\n",
    "    for step in range(max_steps):\n",
    "        action = np.argmax(q_table[state, :])\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        total_rewards += reward\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "print(f\"Average reward over {test_episodes} test episodes: {total_rewards / test_episodes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a4dd42",
   "metadata": {},
   "source": [
    "\n",
    "## Visualization of Q-Table and Policy\n",
    "- **Q-Values Map**: Shows the learned policy in terms of best actions to take in each state.\n",
    "- **Helper Functions**: Functions to create visualizations of the learned Q-values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcab174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "output_dir = \"output\"  # Define the output directory\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "def qtable_directions_map(q_table, map_size):\n",
    "    \"\"\"Get the best learned action & map it to arrows.\"\"\"\n",
    "    qtable_val_max = q_table.max(axis=1).reshape(map_size, map_size)\n",
    "    qtable_best_action = np.argmax(q_table, axis=1).reshape(map_size, map_size)\n",
    "    directions = {0: \"←\", 1: \"↓\", 2: \"→\", 3: \"↑\"}\n",
    "    qtable_directions = np.empty(qtable_best_action.flatten().shape, dtype=str)\n",
    "    eps = np.finfo(float).eps  # Minimum float number on the machine\n",
    "    for idx, val in enumerate(qtable_best_action.flatten()):\n",
    "        if qtable_val_max.flatten()[idx] > eps:\n",
    "            qtable_directions[idx] = directions[val]\n",
    "    qtable_directions = qtable_directions.reshape(map_size, map_size)\n",
    "    return qtable_val_max, qtable_directions\n",
    "\n",
    "def plot_q_values_map(q_table, env, map_size):\n",
    "    \"\"\"Plot the last frame of the simulation and the policy learned.\"\"\"\n",
    "    qtable_val_max, qtable_directions = qtable_directions_map(q_table, map_size)\n",
    "\n",
    "    # Capture the last frame of the environment\n",
    "    env.reset()\n",
    "    frame = env.render()  # Render mode should be set to 'rgb_array'\n",
    "\n",
    "    # Plot the last frame\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "    ax[0].imshow(frame)\n",
    "    ax[0].axis(\"off\")\n",
    "    ax[0].set_title(\"Last Frame\")\n",
    "\n",
    "    # Plot the policy\n",
    "    sns.heatmap(\n",
    "        qtable_val_max,\n",
    "        annot=qtable_directions,\n",
    "        fmt=\"\",\n",
    "        ax=ax[1],\n",
    "        cmap=sns.color_palette(\"Blues\", as_cmap=True),\n",
    "        linewidths=0.7,\n",
    "        linecolor=\"black\",\n",
    "        xticklabels=[],\n",
    "        yticklabels=[],\n",
    "        annot_kws={\"fontsize\": \"xx-large\"},\n",
    "    ).set(title=\"Learned Q-values\\nArrows represent best action\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Instead of plt.show(), save the figure\n",
    "    plot_filename = os.path.join(output_dir, \"q_values_map.png\")\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "\n",
    "def plot_states_actions_distribution(states, actions):\n",
    "    \"\"\"Plot the distributions of states and actions.\"\"\"\n",
    "    labels = {0: \"LEFT\", 1: \"DOWN\", 2: \"RIGHT\", 3: \"UP\"}\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "\n",
    "    # Plot distribution of states\n",
    "    sns.histplot(data=states, ax=ax[0], kde=True, bins=state_space)\n",
    "    ax[0].set_title(\"State Distribution\")\n",
    "\n",
    "    # Plot distribution of actions\n",
    "    sns.histplot(data=actions, ax=ax[1], bins=4)\n",
    "    ax[1].set_xticks(list(labels.keys()), labels=labels.values())\n",
    "    ax[1].set_title(\"Action Distribution\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Instead of plt.show(), save the figure\n",
    "    plot_filename = os.path.join(output_dir, \"states_actions_distribution.png\")\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "\n",
    "# Visualize the Q-table and policy\n",
    "plot_q_values_map(q_table, env, len(custom_map))\n",
    "\n",
    "# Visualize state and action distributions\n",
    "plot_states_actions_distribution(states_visited, actions_taken)\n",
    "\n",
    "# Visualize the Q-table\n",
    "print(\"Trained Q-Table:\")\n",
    "print(q_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
